0it [00:00, ?it/s]  0%|          | 0/170498071 [00:00<?, ?it/s]  0%|          | 8192/170498071 [00:00<34:42, 81884.94it/s]  0%|          | 40960/170498071 [00:00<26:56, 105436.97it/s]  0%|          | 90112/170498071 [00:00<20:37, 137723.88it/s]  0%|          | 221184/170498071 [00:00<15:05, 188101.02it/s]  0%|          | 450560/170498071 [00:00<10:55, 259500.51it/s]  1%|          | 909312/170498071 [00:00<07:48, 361811.51it/s]  1%|          | 1826816/170498071 [00:01<05:31, 508163.00it/s]  2%|▏         | 3678208/170498071 [00:01<03:52, 717368.65it/s]  4%|▍         | 6725632/170498071 [00:01<02:41, 1014089.37it/s]  6%|▌         | 9854976/170498071 [00:01<01:52, 1428551.03it/s]  8%|▊         | 12918784/170498071 [00:01<01:18, 2000212.07it/s]  9%|▉         | 15720448/170498071 [00:01<00:55, 2770793.94it/s] 11%|█         | 18849792/170498071 [00:01<00:39, 3811388.26it/s] 13%|█▎        | 21929984/170498071 [00:01<00:28, 5166446.40it/s] 15%|█▍        | 25010176/170498071 [00:01<00:21, 6878974.84it/s] 16%|█▋        | 28090368/170498071 [00:01<00:15, 8956529.08it/s] 18%|█▊        | 30982144/170498071 [00:02<00:12, 11255795.93it/s] 20%|█▉        | 34004992/170498071 [00:02<00:09, 13855250.01it/s] 22%|██▏       | 37150720/170498071 [00:02<00:08, 16576253.23it/s] 24%|██▎       | 40280064/170498071 [00:02<00:06, 19265125.51it/s] 25%|██▌       | 43425792/170498071 [00:02<00:05, 21632155.49it/s] 27%|██▋       | 46440448/170498071 [00:02<00:05, 23383006.47it/s] 29%|██▉       | 49414144/170498071 [00:02<00:04, 24944417.95it/s] 31%|███       | 52453376/170498071 [00:02<00:04, 26300874.93it/s] 33%|███▎      | 55582720/170498071 [00:02<00:04, 27542670.72it/s] 34%|███▍      | 58695680/170498071 [00:02<00:03, 28400197.65it/s] 36%|███▌      | 61726720/170498071 [00:03<00:03, 28437463.36it/s] 38%|███▊      | 64708608/170498071 [00:03<00:03, 28763912.58it/s] 40%|███▉      | 67772416/170498071 [00:03<00:03, 29286099.42it/s] 42%|████▏     | 70885376/170498071 [00:03<00:03, 29690643.60it/s] 43%|████▎     | 73998336/170498071 [00:03<00:03, 29980828.52it/s] 45%|████▌     | 77111296/170498071 [00:03<00:03, 28680408.94it/s] 47%|████▋     | 80240640/170498071 [00:03<00:03, 29301836.86it/s] 49%|████▉     | 83304448/170498071 [00:03<00:02, 29561660.11it/s] 51%|█████     | 86384640/170498071 [00:03<00:02, 29794504.97it/s] 52%|█████▏    | 89448448/170498071 [00:04<00:02, 29907917.31it/s] 54%|█████▍    | 92577792/170498071 [00:04<00:02, 30180328.80it/s] 56%|█████▌    | 95608832/170498071 [00:04<00:02, 29735398.28it/s] 58%|█████▊    | 98590720/170498071 [00:04<00:02, 29684836.45it/s] 60%|█████▉    | 101588992/170498071 [00:04<00:02, 29771916.08it/s] 61%|██████▏   | 104685568/170498071 [00:04<00:02, 30013512.34it/s] 63%|██████▎   | 107798528/170498071 [00:04<00:02, 30213375.00it/s] 65%|██████▌   | 110911488/170498071 [00:04<00:01, 30350911.57it/s] 67%|██████▋   | 113950720/170498071 [00:04<00:01, 29938060.57it/s] 69%|██████▊   | 116948992/170498071 [00:04<00:01, 29869073.84it/s] 70%|███████   | 119939072/170498071 [00:05<00:01, 29816961.99it/s] 72%|███████▏  | 123052032/170498071 [00:05<00:01, 30166900.62it/s] 74%|███████▍  | 126148608/170498071 [00:05<00:01, 30109131.18it/s] 76%|███████▌  | 129261568/170498071 [00:05<00:01, 30274651.14it/s] 78%|███████▊  | 132341760/170498071 [00:05<00:01, 30306999.82it/s] 79%|███████▉  | 135380992/170498071 [00:05<00:01, 29629908.45it/s] 81%|████████  | 138354688/170498071 [00:05<00:01, 29318633.66it/s] 83%|████████▎ | 141352960/170498071 [00:05<00:00, 29476570.59it/s] 85%|████████▍ | 144416768/170498071 [00:05<00:00, 29683075.36it/s] 87%|████████▋ | 147529728/170498071 [00:05<00:00, 29968408.57it/s] 88%|████████▊ | 150642688/170498071 [00:06<00:00, 30169587.37it/s] 90%|█████████ | 153665536/170498071 [00:06<00:00, 29762196.44it/s] 92%|█████████▏| 156647424/170498071 [00:06<00:00, 29448805.56it/s] 94%|█████████▎| 159703040/170498071 [00:06<00:00, 29645707.00it/s] 95%|█████████▌| 162783232/170498071 [00:06<00:00, 29842467.32it/s] 97%|█████████▋| 165912576/170498071 [00:06<00:00, 30133956.73it/s] 99%|█████████▉| 169025536/170498071 [00:06<00:00, 30291183.25it/s]170500096it [00:09, 17284410.68it/s]                               
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())
Traceback (most recent call last):
  File "rocl_train.py", line 214, in <module>
    train_loss, reg_loss = train(epoch)
  File "rocl_train.py", line 160, in train
    loss.backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [512]] is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'rocl_train.py', '--local_rank=0', '--ngpu', '1', '--batch-size=256', '--model=ResNet18', '--k=7', '--loss_type=sim', '--advtrain_type=Rep', '--attack_type=linf', '--regularize_to=other', '--attack_to=other', '--train_type=contrastive', '--dataset=cifar-10']' returned non-zero exit status 1.
